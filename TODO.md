# TODO

## Planned

- [ ] JSON output mode (`--json` flag) for programmatic consumption
- [ ] CSV export for spreadsheet analysis
- [ ] Custom prompt support (`--prompt` flag or `BENCHMARK_PROMPT` env var)
- [ ] Configurable benchmark duration / token limit
- [ ] Multi-round benchmarking with statistical confidence intervals
- [ ] GPU utilization metrics (requires Ollama API support)
- [ ] Model pull suggestions for untested tiers
- [ ] Warm-up run before benchmarking to eliminate cold-start bias
- [ ] Markdown report generation (`--report` flag)

## Ideas

- [ ] Web UI dashboard for benchmark results
- [ ] Historical result tracking and comparison
- [ ] Model recommendation based on specific use case (coding, chat, RAG)
- [ ] Parallel benchmarking across multiple hosts
- [ ] Docker container with pre-built binaries
- [ ] mDNS/Bonjour discovery as alternative to subnet scanning
